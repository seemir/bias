\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Methodology}
\label{chap:method}

\noindent In this chapter we present the methodology and research structure used in this thesis. Some pre-processing of data including imputation and dimensional reduction will also be explained and presented. The implementation of the ML algorithms that produce the results presented in chapter (\ref{chap:exp}).

\section{Overview}

\noindent As stated in the chapter (\ref{chap:intro}), the aim of the thesis is split into two parts. The first part being that of seeing how well various clustering methods perform in producing phenotypically distinct clinical patient groups with HFpEF and HFmrEF? We frame the SL problem in the setting of unsupervised learning and accordingly use the following clustering methods: hierarchical clustering, k-Means and expectation-maximization to evaluate which produce the most clinically useful patient groups. The use of these clustering methods are common in the literature (see section \ref{subsec:unsupervisedlearn}) and serves as the main motivation for including them in our analysis. The second part of the problem statement looks at evaluating the accuracy of various classification algorithms in predicting the mortality and re-admission of patients with HFpEF and HFmrEF? In accordance with the literature as presented in section (\ref{sec:predclincout}), we reduce the SL problem of predicting the mortality and re-admission into a two class classification problem where both classes of outcomes are whether or not mortality/re-admission occurred. The classification methods that we will be evaluating are: k-nearest neighbours (k-NN), support vector machines (SVM), random forest and least absolute shrinkage and selection operator (LASSO) algorithms. All the algorithms are much used in the literature. The motivation behind the use of the chosen algorithms has always been to confirm with the practices done in the literature. We do however need to emphasize that many algorithms exists that can be used to further broaden the analysis done in this thesis. This is something we have not done due to limitations.

\input{doc/thesis/images/Research_structure.tex}

\indent The machine learning procedure adopted in this thesis is illustrated in figure (\ref{fig:ML_proc_thesis}). The structure starts by pre-processing the data. This includes imputation of missing data to ensure that the data is balanced and dimensional reduction to address eventual problems with higher dimensional multi-correlated variables. Both the pre-processing and the dimensional reduction steps are explained in further detail later in this chapter (see section \ref{sec:data}). After the pre-processing is done the structure continues by first addressing the cluster analysis. Being that the dimension reduction is relevant for both the cluster analysis and classification. We use the components derived from the dimension reduction process as input into both the clustering and classification algorithms evaluated. The cluster analysis runs the produced components through the three cluster algorithms (hierarchical clustering, k-Means and expectation maximization). After the procedure is done, three sets of clusters are produced and the next step is to evaluate the clusters to assess their medical usefulness. The supervised classification track on the other hand is structured in a somewhat different way. First the procedure starts by removing the labels of the various clinical outcomes associated with the given patients. After this is done the chosen components from the dimension reduction is run through the four classification algorithms (k-NN, SVM, RF and LASSO) and the data is trained and validated to produce approximately unbiased estimates of the test errors/accuracy. After the data is run thought the classification process and the accuracy are produced, the algorithms are ranked and evaluated accordingly. The outputs of the whole ML procedure are i) clinical clusters that \textit{may} have distinct phenotypical properties and ii) the accuracy of the various classification algorithms in predicting re-admission and mortality in the data sets.\\
\indent All the processes mentioned in the ML procedure in figure (\ref{fig:ML_proc_thesis}) are developed using the \texttt{R} statistical programming language (version 3.4.4 - \textit{Someone to Lean On}) \citep{Rsoftware2018} with RStudio as the integrated development environment (IDE), version 1.1.423 \citep{RStudio2018}. We use a number of external libraries and self-made algorithms in order to make the process more efficient. Data description with variable explanations, descriptive statistics and some relevant plot can be fount in appendix (\ref{chap:data_desc}). The source code used to produce all the results in this thesis can be found in appendix (\ref{chap:souce_code}). As we now have given an overview of the ML procedure used in this thesis we move on to presenting the data.

\section{Data}
\label{sec:data}

\noindent The data used is comprised of two data sets (\texttt{data\_use\_HFpEF.mat}, $N = 182$ and \texttt{data\_use\_HFmrEF.mat}, $N = 193$). The data was collected by the medical staff at a tertiary hospital in the United Kingdom. At this particular hospital NT-proBNP led heart failure service were run on all patients with suspected heart failure. All patients with suspected HF based on an assessment of the HF probability and raised NT-proBNP/BNP levels (see figure \ref{fig:esc_algo_hf}) were included and forwarded for an echocardiography. An expert HF physician reviewed all the patients after the echocardiography was performed. The patients were diagnosed with HF according to the 2016 ESC guidelines \citep{ponikowski2016}. Accordingly, signs and symptoms of HF, raised NP values, echocardiographic results including left ventricular ejection fraction (LVEF) and evidence of structural or functional heart abnormalities were the primary basis for the assessment done by the hospitals cardiac physicians. After the diagnosis patients were categorized based on LVEF following the ESC guidelines, i.e. patients with LVEF $>$ 50\% were classified as HFpEF and those with $40 \leq$ LVEF $< 50$ as HFmrEF. The patients with LVEF $<$ 40\%, greater than moderate valvular heart disease and prior cardiac transplantation were excluded. The data was collected over a one year period from October 10th, 2014 to October 9th, 2015. In total 375 patients were analyzed over this one year period with data from 126 clinical features being recorded. The outcomes were evaluated through the hospitals databases and mortality was confirmed with the Office for National Statistics. All the data was collected as part of the hospitals approved Clinical Audit.\\
\indent As mentioned in the previous section, we reduced the SL problem in the supervised learning part of the ML procedure to that of a two class classification problem. The way in which this was done was with respect to the various \texttt{patient\_groups} in the data. The patients were grouped based on various outcomes. In total six outcome categories were defined in the 

\input{doc/thesis/tables/Outcome_classes.tex}

\noindent data sets. The outcome categories are as follows: \texttt{IN} - inhospital death, \texttt{Z} \noindent dead within 30 days, \texttt{Y} - dead within 1 year, \texttt{X} - dead by Fluorouracil (medication), \texttt{V} - cardiac readmission within 30 days, \texttt{U} - readmission and \texttt{R} - the rest. The various combinations of the outcome classes found in the data sets and the way in which they were classified are listed in table (\ref{tab:outcomes_class}). From this table we can see that approximately 36\% of all the patients in the HFpEF data set were readmitted in some form, i.e either within 30 days or more. In the HFmrEF data set this number was somewhat smaller being approximately 23\%. The number also differed with respect to the whether the patients were confirmed deceased or not. In the HFpEF data set approximately 30\% of the patients were confirmed deceased and in the HFmrEF data set this number was 31\%. Further descriptive statistics on the data can be found in appendix (\ref{sec:desc_stat}). The source code for the two-class outcome classification shown in table (\ref{tab:outcomes_class}) can be found in appendix (\ref{sec:app_desc_stat}).\\
\indent As the data used in this thesis is cross-sectional, we need to emphasize that it is not perfect. Limitations to the data sets are many and one of the most primary of them is in regards to missing data.


\subsection{Missing Data}
\label{subsec:miss_data}

\noindent Missing values in data is a very important concept in data management and a highly prevalent problem in any data analysis. If one does not handle missing values properly this may lead to inaccurate or invalid inference being drawn from the data. Results where improper treatment of missing data is present may differ significantly from those where missing data is not present. In medical research it is not uncommon for patient data to be missing. Missing data from patients clinical variables are typically defined as the values that are not directly observed \citep{ibrahim2012missing}. Data can be missing due to a number of reasons. In clinical research some reasons may include: poor communication with study subject, difficulties assessing the clinical outcomes, lack of consolidation from test, duration of trial etc. The latter is often a reason for missing data as longer trials tend to produce more risk of missing data. Especially considering that patients often run the risk of being dropped from the studies before completion \citep{myers2000handling}.\\
\indent In our data sets the problem with missing values is very much present. In the HFpEF data set a total of 2238 observations, i.e. approximately 12.5\% of the data set is missing. In the non-indicator variables, the largest contributors to this can be attributed to the lack of glucose measured (\texttt{glucose}, 6.1\%), failure to register time to HF admission (\texttt{timetohfadm}, 5.5\%), ferritin levels (\texttt{ferritin}, 5.5\%), lateral peak velocity of systolic pulmonary vein flow (\texttt{laterals}, 5.3\%), glycohemoglobin (\texttt{hba1c}, 5.0\%) and transferrin saturation (\texttt{tsat}, 4.4\%). These variables contribute to approximately 31.8\% of the total number of missing values. In the HFmrEF data set, the picture is very much different. In general we can say that this data set has a much  

\input{doc/thesis/tables/Missing_values.tex}

\noindent larger presence of missing values. In total 3586 observations, i.e. approximately 22.4\% of the data is missing. The largest non-indicator contributors are: inability to record the body mass index (BMI) at admission (\texttt{bmiadmission}, 5\%), the weight after patients are discharged (\texttt{dischargewe- ight}, 4.9\%), the number of procedures done (\texttt{procedures}, 4.9\%), measures of troponin (\texttt{troponin}, 3.9\%), the time to first cardiac hospitalization (\texttt{timetofirstcardiachospitalisation}, 3.9\%) and the patients weight at admission (\texttt{admissionwgt}, 3.7\%). These variables account for 26.3\% of the total number of missing values in the HFmrEF data. An overview of the variables with the most missing values is shown in table (\ref{tab:top_missing}).

\subsection{Little's Test for MCAR}

\noindent The presence of missing values is something that needs to be addressed by any individual conducting data analysis. As missing values makes the data corrupted and may introduce statistical bias that may lead to invalid results and inferences. It is also vital for us as many of the statistical methods used later in this thesis cannot be applied in the presence of missing values. When talking about missing values one typically mentions three distinct types of missing values, see e.g. \cite{sterne2009multiple} and \cite{kaushal2014missing} for further explanation. These are as follows:
\begin{enumerate}[label=(\roman*)]
    \item Missing completely at random (MCAR): This type assume that there is no systematic difference between the missing values and the observed values. Some example can be if blood pressure values are missing due to breakdown in automatic sphygmomanometer or if blood sugar values are missing due to non working glucometer.
    \item Missing at random (MAR): The second type of missing values assume that any difference between the missing values and the observed values can be explained by differences in the observed values. Again an example can be that missing blood pressure values or blood sugar values may be lower than the measured values but only because younger people may be more likely to have missing blood pressure and blood sugar as missing.
    \item Missing not at random (MNAR): The last and final type assumes that even after the observer data are taken into account, the systematic differences between the observed and missing values are still present. An example can be that people with high values of blood pressure or blood sugar may be less likely to go to appointment due to headache.  
\end{enumerate}

\noindent The last type of missing value can only be speculated and thus never determined, see e.g. \cite{rubin1976inference}, \cite{schafer2002missing} and \cite{moons2006using}. In our data, we assume that the missing data is at least missing at random (MAR). This is an assumption that many in the literature place on their data without an attempt at supplying some arguments to support such an assumption. To this we have carried out Little's MCAR test \citep{little1988test} on our data (separately on indicator and continuous variables). The test is structured with the following three steps : 
\begin{enumerate}[label=(\roman*)]
    \item First the test starts by using the expectation-maximization (EM) algorithm \citep{dempster1977maximum} to estimate the maximum likelihood of the population mean $\bm{\tilde{\mu}}_{obs, j}$ and variance-covariance matrix $\bm{\tilde{\Sigma}}_{obs,j}$. Here one enters the $Y:N\times p$ matrix of data into the EM algorithm.
    \item Next step is to create a set of matrices $S_j$ for $j = 1, \hdots, J$ where each matrix of the dataset consists of all cases that are identified with particular missing patterns (0 = not-missing and 1 = missing). Define $m_j$ to be the number of cases that belong to a given missing response pattern in $S_j$. From these $J-1$ cases, calculate the \textit{observed} vector of means $\bm{\hat{y}}_{obs, j}$ for each random response pattern. 
    \item The final step comprises of calculating the difference between the observed means in step 2 with the estimated EM-means from step 1 weighted by $m_j$ and the inverse variance-covariance matrix to obtain the the following test statistics:
    \begin{align}
        d^2 = \sum_{j=1}^J m_j \left(\bm{\hat{y}}_{obs, j} -  \bm{\tilde{\mu}}_{obs, j}\right)\bm{\tilde{\Sigma}}_{obs,j}^{-1}\left(\bm{\hat{y}}_{obs, j} -  \bm{\tilde{\mu}}_{obs, j}\right)^T
    \end{align}
\end{enumerate}

\noindent \cite{little1988test} showed that $d^2$ is asymptotically $\chi^2$-distributed with $f = \sum_{j=1}^J p_j - p$ degrees of freedom, where $p_j$ is the number of observed variables for cases in $S_j$. Thus with the use of $d^2$, a large-sample test of the MCAR assumption compares $d^2$ with a chi-squared distribution with $f$ df can be done, and rejecting the null hypothesis when $d^2$ is large. Following this procedure, we have carried out Little's MCAR test and the results are presented in table (\ref{tab:little_test}). The results were produces using the function \texttt{LittleMCAR()} in the \texttt{r} package \texttt{BaylorEdPsych} \citep{BaylorEdPsych}. We removed the variables that had more than 15\% missing values from the HFpEF dataset and 20\% from the HFmrEF dataset (see table \ref{tab:top_missing} for top 10 missing variables). Next, we split the variables into to two datasets, one for the continuous variables and one for the indicator variables. We also removed the variables that had near zero variance using the \texttt{nearZeroVar()} function in the \texttt{caret} package \citep{kuhncaret}. As remarked by \cite{BaylorEdPsych}, the \texttt{LittleMCAR()} function can be very time inefficient for datasets with more than 50 variables. This time inefficiency is something we have tried to address by splitting the datasets into the two subsets and thus conduct separate tests on both subsets. The test assumes that the data is MCAR, and this is accordingly the null-hypothesis. From table (\ref{tab:little_test}), we can see that all the $p$-values are insignificant at 5\% significance level. Suggesting that we cannot reject the null hypothesis of the missing data being MCAR. However, as argued by \cite{allison1999missing}, just because the data passes this test does not mean that the MCAR assumption is satisfied. The assumptions for MCAR are strong and a simple test such as the one suggested by \cite{little1988test}, does not in and of itself, satisfy those assumptions. It merely lends evidence in its support and given the test results presented in table (\ref{tab:little_test}) we consider this assumption to be intact.\\
\indent When it comes to the question regarding missing values there exists many ways of dealing with this problem. Each of these ways have different disadvantages as well as advantages. One of the most used ways of dealing with missing values is through the use of imputation techniques. 

\input{doc/thesis/tables/MCAR_test.tex}

\subsection{Imputation}
\label{subsec:impu}

\noindent There exists a wide variety of methods that fall under the class of imputation. In general all methods that attempt to replace each missing value in a dataset with an estimate or a guess are typically classified as being an imputation method \citep{allison1999missing}. A very populate and conventional method of imputing missing values is through the use of mean imputation. This methods implies swapping each missing value with the mean on the observed values in the given variable. 




\subsection{Dimensional Reduction}
\label{subsec:dim_red}

\section{Clustering Patient Groups}
\label{sec:cluster_pat_gro}

\subsection{Hierarchical}
\label{subsec:hierarchical}

\subsection{k-Means}
\label{subsec:k-means}

\subsection{Expectation-Maximization}
\label{subsec:em}

\section{Classifying Clinical Outcomes}
\label{sec:classify_clin_out}

\subsection{k-Nearest Neighbours (k-NN)}
\label{subsec:knn}

\subsection{Support Vector Machines (SVM)}
\label{subsec:svm}

\subsection{Random Forrest}
\label{subsec:random_forr}

\subsection{LASSO}
\label{subsec:lasso}

\section{Validation}
\label{sec:validation}

\subsection{k-Fold}
\label{subsec:k_fold}

\subsection{Leave One Out}
\label{subsec:loocv}

\end{document}