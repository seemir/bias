\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Methodology}
\label{chap:method}

\noindent In this chapter we present the methodology and research structure used in this thesis. Some pre-processing of data including imputation and dimensional reduction will also be explained and presented. The implementation of the ML algorithms that produce the results presented in chapter (\ref{chap:exp}).

\section{Overview}

\noindent As stated in the chapter (\ref{chap:intro}), the aim of the thesis is split into two parts. The first part being that of seeing how well various clustering methods perform in producing phenotypically distinct clinical patient groups with HFpEF and HFmrEF? We frame the SL problem in the setting of unsupervised learning and accordingly use the following clustering methods: hierarchical clustering, k-Means and expectation-maximization to evaluate which produce the most clinically useful patient groups. The use of these clustering methods are common in the literature (see section \ref{subsec:unsupervisedlearn}) and serves as the main motivation for including them in our analysis. The second part of the problem statement looks at evaluating the accuracy of various classification algorithms in predicting the mortality and re-admission of patients with HFpEF and HFmrEF? In accordance with the literature as presented in section (\ref{sec:predclincout}), we reduce the SL problem of predicting the mortality and re-admission into a two class classification problem where both classes of outcomes are whether or not mortality/re-admission occurred. The classification methods that we will be evaluating are: k-nearest neighbours (k-NN), support vector machines (SVM), random forest and least absolute shrinkage and selection operator (LASSO) algorithms. All the algorithms are much used in the literature. The motivation behind the use of the chosen algorithms has always been to confirm with the practices done in the literature. We do however need to emphasize that many algorithms exists that can be used to further broaden the analysis done in this thesis. This is something we have not done due to limitations.

\input{doc/thesis/images/Research_structure.tex}

\indent The machine learning procedure adopted in this thesis is illustrated in figure (\ref{fig:ML_proc_thesis}). The structure starts by pre-processing the data. This pre-processing step consists of three sub processes: consolidation, imputation and dimension reduction. The consolidation process merges the HFpEF and HFmrEF data set into one data set with the same types of variables. In addition to having one data set with all the observations, the process also leaves the data separate (but with equal variables) so that an analysis on each separate data set can be done. Furthermore, the clinical outcomes of the patients in the data set are extracted by this process and stored for later use in the classification part of thesis. The imputation process does imputation of missing data to ensure that the data is balanced and the dimensional reduction process addresses eventual problems with higher dimensional multi-correlated variables. The pre-processing steps are explained in further detail later in this chapter (see section \ref{sec:data}). After the pre-processing is done the structure continues by first addressing the cluster analysis. Being that the dimension reduction is relevant for both the cluster analysis and classification. We use the components derived from the dimension reduction process as input into both the clustering and classification algorithms evaluated. The cluster analysis runs the produced components through the three cluster algorithms (hierarchical clustering, k-Means and expectation maximization). After the procedure is done, three sets of clusters are produced and the next step is to evaluate the clusters to assess their medical usefulness. The supervised classification track on the other hand is structured in a somewhat different way. First the procedure starts by running the chosen components from the dimension reduction through the four classification algorithms (k-NN, SVM, RF and LASSO) and the data is trained and validated to produce approximately unbiased estimates of the test errors/accuracy. After the data is run thought the classification process and the accuracy are produced, the algorithms are ranked and evaluated accordingly. The outputs of the whole ML procedure are i) clinical clusters that \textit{may} have distinct phenotypical properties and ii) the accuracy of the various classification algorithms in predicting re-admission and mortality in the data sets. All the processes mentioned in the ML procedure in figure (\ref{fig:ML_proc_thesis}) are developed using the \texttt{R} statistical programming language (version 3.4.4 - \textit{Someone to Lean On}) \citep{Rsoftware2018} with RStudio as the integrated development environment (IDE), version 1.1.423 \citep{RStudio2018}. We use a number of external libraries and self-made algorithms in order to make the process more efficient. Data description with variable explanations, descriptive statistics and some relevant plot can be fount in appendix (\ref{chap:data_desc}). The source code used to produce all the results in this thesis can be found in appendix (\ref{chap:souce_code}). As we now have given an overview of the ML procedure used in this thesis we move on to presenting the data.

\section{Data}
\label{sec:data}

\noindent The data used is comprised of two data sets (\texttt{data\_use\_HFpEF.mat}, dim: $193 \times 92$ and \texttt{data\_use\_HFmrEF.mat}, dim: $182 \times 87$). Being that both data sets had different types of clinical variables, we consolidated the data into three data set with the same number and types of variables: the full sample (\texttt{HFfulldata set.Rdat}, dim: $374 \times 55$), the HFpEF sample (\texttt{HFpEFdata set .Rdat}, dim: $193 \times 92$) and the HFmrEF sample (\texttt{HFmrEFdata set.Rdat}, dim: $182 \times 87$).\\
\indent The data was collected by the medical staff at a tertiary hospital in the United Kingdom. At this particular hospital NT-proBNP led heart failure service were run on all patients with suspected heart failure. All patients with suspected HF based on an assessment of the HF probability and raised NT-proBNP/BNP levels (see figure \ref{fig:esc_algo_hf}) were included and forwarded for an echocardiography. An expert HF physician reviewed all the patients after the echocardiography was performed. The patients were diagnosed with HF according to the 2016 ESC guidelines \citep{ponikowski2016}. Accordingly, signs and symptoms of HF, raised NP values, echocardiographic results including left ventricular ejection fraction (LVEF) and evidence of structural or functional heart abnormalities were the primary basis for the assessment done by the hospitals cardiac physicians. After the diagnosis patients were categorized based on LVEF following the ESC guidelines, i.e. patients with LVEF $>$ 50\% were classified as HFpEF and those with $40 \leq$ LVEF $< 50$ as HFmrEF. The patients with LVEF $<$ 40\%, greater than moderate valvular heart disease and prior cardiac transplantation were excluded. The data was collected over a one year period from October 10th, 2014 to October 9th, 2015. In total 375 patients were analyzed over this one year period with data from 126 clinical features being recorded. The outcomes were evaluated through the hospitals databases and mortality was confirmed with the Office for National Statistics. All the data was collected as part of the hospitals approved Clinical Audit. As mentioned

\input{doc/thesis/tables/Outcome_classes.tex}

\newpage

\noindent in the previous section, we reduced the SL problem in the supervised learning part of the ML procedure to that of a two class classification problem. The way in which this was done was with respect to the various \texttt{patient\_groups} in the data. The patients were grouped based on various \noindent outcomes. In total six outcome categories were  defined in the data sets. The outcome categories are as follows: \texttt{IN} - inhospital death, \texttt{Z} \noindent dead within 30 days, \texttt{Y} - dead within 1 year, \texttt{X} - dead by Fluorouracil (medication), \texttt{V} - cardiac readmission within 30 days, \texttt{U} - readmission and \texttt{R} - the rest. The various combinations of the outcome classes found in the data sets and the way in which they were classified are listed in table (\ref{tab:outcomes_class}).\\
\indent From this table we can see that approximately 36.8\% of all the patients in the HFpEF data set were readmitted in some form, i.e either within 30 days or more. In the HFmrEF data set this number was somewhat smaller being approximately 23.4\%. In the full sample approximately 29.1\% of the patients were readmitted. The number also differed with respect to the whether the patients were confirmed deceased or not. In the HFpEF data set approximately 29.9\% of the patients were confirmed deceased and in the HFmrEF data set this number was 31.1\%. For the full sample this number is approximately 30.7\%. Further descriptive statistics on the data can be found in appendix (\ref{sec:desc_stat}). The source code for the two-class outcome classification shown in table (\ref{tab:outcomes_class}) can be found in appendix (\ref{sec:app_desc_stat}). As the data used in this thesis is cross-sectional, we need to emphasize that it is not perfect. Limitations to the data sets are many and one of the most primary of them is regarding missing data. 

\subsection{Missing Data}
\label{subsec:miss_data}

\noindent Missing values in data is a very important concept in data management and a highly prevalent problem in any data analysis. If one does not handle missing values properly this may lead to inaccurate or invalid inference being drawn from the data. Results where improper treatment of missing data is present may differ significantly from those where missing data is not present. In medical research it is not uncommon for patient data to be missing. Missing data from patients clinical variables are typically defined as the values that are not directly observed \citep{ibrahim2012missing}. Data can be missing due to a number of reasons. In clinical research some reasons may include: poor communication with study subject, difficulties assessing the clinical outcomes, lack of consolidation from test, duration of trial etc. The latter is often a reason for missing data as longer trials tend to produce more risk of missing data. Especially considering that patients often run the risk of being dropped from the studies before completion \citep{myers2000handling}.

\input{doc/thesis/tables/Missing_values.tex}

\indent In our data sets the problem with missing values is very much present. In the full data set, a total of 3081 observations are missing accounting for about 14.9\% of the total data set. The main non-indicator variables accounting for the highest amount of this number is the lack of registering ferritin levels (\texttt{ferritin}, 8.1\% of missing), BMI at admission (\texttt{bmiadmission}, 7.2\%), ironlevels (\texttt{ironlevels}, 6.8\%), transferrin saturation (\texttt{tsat}, 6.8\%), time of HF admission (\texttt{timetohfadm}, 6\%), pulmonary artery systolic pressure (\texttt{pasp}, 5.9\%), weight at admission (\texttt{admissionwgt}, 5.3\%) and ECQ QRS duration (4.6\%). We can also look at the missing values in both the sub data sets. In the HFpEF data set a total of 973 observations, i.e. approximately 9.2\% of the data set is missing. Of the non-indicator variables, the largest contributors can be attributed to the failure of registering time to HF admission (\texttt{timetohfadm}, 12.7\% of missing), ferritin levels (\texttt{ferritin}, 12.5\%), transferrin saturation (\texttt{tsat}, 10.2\%), iron levels (\texttt{ironlevels}, 10.1\%), pulmonary artery systolic pressure (\texttt{pasp}, 7.3\%), registering body-mass-index (BMI) at admission (\texttt{bmiadmission}, 4.6\%), E/e' ratio (\texttt{ee}, 4.2\%), ECQ QRS duration (\texttt{ecgqrsduration}, \texttt{3.7\%}) and ECG rate (\texttt{ecgrate}, 3.5\%). These variables contribute to approximately 68.8\% of the missing values in the HFpEF data. In the HFmrEF data set, the picture is very much different. In general we can say that this data set has a much larger presence of missing values. Even as the clinical variables used in both sets are the same. In total 2108 observations, i.e. approximately 21.1\% of the data is missing. The largest non-indicator contributors are: inability to record the body mass index (BMI) at admission (\texttt{bmiadmission}, 8.4\%), the weight of patients at admission (\texttt{admissionwgt}, 6.2\%), ferritin levels (\texttt{ferritin}, 6.1\%), iron levels (\texttt{ironlevels}, 5.3\%), transferrin saturation (\texttt{tsat}, 5.3\%), pulmonary artery systolic pressure (\texttt{pasp}, 5.2\%) and ECQ QRS duration (\texttt{ecgqrsduration}, \texttt{5\%}). These variables account for 41.1\% of the missing values in the HFmrEF data. An overview of the variables with the most missing values in each data set can be found in table (\ref{tab:top_missing}).

\subsection{Little's Test for MCAR}
\label{subsec:little}

\noindent The presence of missing values is something that needs to be addressed by any individual conducting data analysis. As missing values makes the data corrupted and may introduce statistical bias that may lead to invalid results and inferences. This is vital for us as many of the statistical methods used later in this thesis cannot be conducted in the presence of missing values. When talking about missing values one typically mentions three distinct types of missing values, see e.g. \cite{sterne2009multiple} and \cite{kaushal2014missing} for further explanation. These are as follows:

\newpage

\begin{enumerate}[label=(\roman*)]
    \item Missing completely at random (MCAR): This type assume that there is no systematic difference between the missing values and the observed values. Some example can be if blood pressure values are missing due to breakdown in automatic sphygmomanometer or if blood sugar values are missing due to non working glucometer.
    \item Missing at random (MAR): The second type of missing values assume that any difference between the missing values and the observed values can be explained by differences in the observed values. Again an example can be that missing blood pressure values or blood sugar values may be lower than the measured values but only because younger people may be more likely to have missing blood pressure and blood sugar as missing.
    \item Missing not at random (MNAR): The last and final type assumes that even after the observer data are taken into account, the systematic differences between the observed and missing values are still present. An example can be that people with high values of blood pressure or blood sugar may be less likely to go to appointment due to headache.  
\end{enumerate}

\noindent The last type of missing value can only be speculated and thus never determined, see e.g. \cite{rubin1976inference}, \cite{schafer2002missing} and \cite{moons2006using}. In our data, we assume that the missing data is at least missing at random (MAR). This is an assumption that many in the literature place on their data without an attempt at supplying some arguments to support such an assumption. To this we have carried out Little's MCAR test \citep{little1988test} on our data (separately on indicator and continuous variables). The test is structured with the following three steps : 
\begin{enumerate}[label=(\roman*)]
    \item First the test starts by using the expectation-maximization (EM) algorithm \citep{dempster1977maximum} to estimate the maximum likelihood of the population mean $\bm{\tilde{\mu}}_{obs, j}$ and variance-covariance matrix $\bm{\tilde{\Sigma}}_{obs,j}$. Here one enters the $Y:N\times p$ matrix of data into the EM algorithm.
    \item Next step is to create a set of matrices $S_j$ for $j = 1, \hdots, J$ where each matrix of the data set consists of all cases that are identified with particular missing patterns (0 = not-missing and 1 = missing). Define $m_j$ to be the number of cases that belong to a given missing response pattern in $S_j$. From these $J-1$ cases, calculate the \textit{observed} vector of means $\bm{\hat{y}}_{obs, j}$ for each random response pattern. 
    \item The final step comprises of calculating the difference between the observed means in step 2 with the estimated EM-means from step 1 weighted by $m_j$ and the inverse variance-covariance matrix to obtain the the following test statistics:
    \begin{align}
        d^2 = \sum_{j=1}^J m_j \left(\bm{\hat{y}}_{obs, j} -  \bm{\tilde{\mu}}_{obs, j}\right)\bm{\tilde{\Sigma}}_{obs,j}^{-1}\left(\bm{\hat{y}}_{obs, j} -  \bm{\tilde{\mu}}_{obs, j}\right)^T
    \end{align}
\end{enumerate}

\noindent \cite{little1988test} showed that $d^2$ is asymptotically $\chi^2$-distributed with $f = \sum_{j=1}^J p_j - p$ degrees of freedom, where $p_j$ is the number of observed variables for cases in $S_j$. Thus with the use of $d^2$, a large-sample test of the MCAR assumption compares $d^2$ with a chi-squared distribution with $f$ df can be done, and rejecting the null hypothesis when $d^2$ is large. Following this procedure, we have carried out Little's MCAR test and the results are presented in table (\ref{tab:little_test}). The results were produces using the function \texttt{LittleMCAR()} in the \texttt{r} package \texttt{BaylorEdPsych} \citep{BaylorEdPsych}. We removed the variables that had more than 15\% missing values from the

\input{doc/thesis/tables/MCAR_test.tex} 

\noindent HFpEF data set, 25\% from the HFmrEF data set and 20\% from the full data set (see table \ref{tab:top_missing} for top 10 missing variables). Next, we split the variables into to two data sets, one for the continuous variables and one for the indicator variables. We also removed the variables that had near zero variance using the \texttt{nearZeroVar()} function in the \texttt{caret} package \citep{kuhncaret}. As remarked by \cite{BaylorEdPsych}, the \texttt{LittleMCAR()} function can be very time inefficient for data sets with more than 50 variables. This time inefficiency is why we split the data sets into the two subsets, i.e. continuous and indicator and thus conducted separate tests on both subsets. The test assumes that the data is MCAR, and this is accordingly the null-hypothesis. From table (\ref{tab:little_test}), we can see that all the $p$-values are insignificant at 5\% significance level. Suggesting that we cannot reject the null hypothesis of the missing data  
\noindent being MCAR. However, as argued by \cite{allison1999missing}, just because the data passes this test does not mean that the MCAR assumption is satisfied. The assumptions for MCAR are strong and a simple test such as the one suggested by \cite{little1988test}, does not in and of itself satisfy those assumptions. It merely lends evidence in its support and given the test results presented in table (\ref{tab:little_test}) we consider this assumption to be intact. When it comes to the question regarding missing values there exists many ways of dealing with this problem. Each of these ways have different disadvantages as well as advantages. One of the most used ways of dealing with missing values is through the use of imputation techniques. 

\subsection{Imputation}
\label{subsec:impu}

\noindent There exists a wide variety of methods that fall under the class of imputation. In general all methods that attempt to replace each missing value in a data set with an estimate or a guess are typically classified as being an imputation method \citep{allison1999missing}. A very popular and conventional method of imputing missing values is through the use of mean imputation. This method implies swapping each missing value with the mean of the observed values in the given variable column. The method is very easy to use and maintains the sample size, but it has a problem with underestimating both the variance and standard deviation estimates. This implies that the estimates that the produced imputed values are unbiased see e.g. \cite{scheffer2002dealing}, \cite{enders2010applied} and \cite{eekhout2012brief}. Another class of imputation methods that have proven to handle missing values in a wide variety of cases is the maximum likelihood methods. The use of set method requires that the assumption of MCAR is intact and as such if this is done can produce estimates that have the desirable properties normally associated with maximum likelihood. These properties are consistency (estimates will be approximately unbiased in large samples), asymptotic efficiency (estimates are close to being fully efficient i.e., having minimal standard errors) and asymptotic normality (allows the use of normal approximation to calculate confidence intervals and $p$-values). Additionally, the use of maximum likelihood methods can produce standard errors that fully account for the fact that some data is missing \citep{allison1999missing}. Its exactly based on these qualities that we have chosen maximum likelihood based imputation as one of the strategies to address the problem with the missing values in our data set presented in subsection (\ref{subsec:miss_data}). We also showed that this is relevant as the assumption of MCAR is assumed intact, see subsection (\ref{subsec:little}).\\
\indent A maximum likelihood method typically starts out by expressing a likelihood function. This function expresses the probability of the data as a function of the unknown parameters. If we assume two discrete random variables: $\mathbf{X}$ and $\mathbf{Z}$ with a joint probability function defined by $p(x,z|\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ is a vector of parameters. This joint probability function gives us the probability that $\mathbf{X} = x$ and $\mathbf{Z} = z$. If we assume that there are no missing values and that the observations are independent, i.e. $cov(\mathbf{X}, \mathbf{Z}) = 0$. Then the likelihood function is defined by:
\begin{align}
    L(\boldsymbol{\theta}) = \prod_{i=1}^n p(x_i, z_i | \boldsymbol{\theta})
    \label{eq:likelihood}
\end{align}
\noindent To find an estimate of the maximum likelihood, we need to find the value for $\boldsymbol{\theta}$ that maximizes the likelihood function (eq. \ref{eq:likelihood}). This can be done using the log-likelihood function ($\mathcal{L} (\boldsymbol{\theta}) = \log L(\boldsymbol{\theta})$) and should give us an estimate defined by:
\begin{align}
    {\displaystyle {\hat {\theta }}\in \left \{{\underset {\theta \in \Theta }{\operatorname {arg\,max} }}\ \sum_{i=1}^n\log p (x_i, z_i | \boldsymbol{\theta})\right \}}
\end{align}
\noindent If we assume that the data is MAR on $\boldsymbol{Z}$ for the first $r$ cases, and MAR on $\boldsymbol{X}$ for the next $s$ cases. Then we can split the likelihood function into parts that correspond to each missing value pattern and accordingly factor these parts. This in order to get a likelihood function that takes into account the missing data patterns. Then the likelihood function becomes:
\begin{align}
    L(\boldsymbol{\theta}) = \prod_{i=1}^r g(x_i | \boldsymbol{\theta})\prod_{i=r+1}^{r+s} h(z_i | \boldsymbol{\theta})\prod_{i=r+s+1}^n p(x_i, z_i | \boldsymbol{\theta}) 
\end{align}
\noindent where $g(x | \theta)$ and $h(z | \theta)$ are the marginal distributions of $\boldsymbol{X}$ and $\boldsymbol{Z}$, so that:
\begin{align}
    \prod_{i = 1}^r g(x_i | \boldsymbol{\theta}) \prod_{i = r + 1}^{r + s} h(z_i | \boldsymbol{\theta}) = \prod_{i=1}^{r+s} p(x_i, z_i | \boldsymbol{\theta})
\end{align}
\noindent For each missing data pattern, the likelihood is found by summing the joint distribution over all possible values of the variables with missing data. So the estimated maximum likelihood parameters in this particular example should be defined by:
\begin{align}
    {\displaystyle {\hat {\theta }}\in \left \{{\underset {\theta \in \Theta }{\operatorname {arg\,max} }}\ \left(\sum_{i=1}^{r+s}\log p(x_i, z_i | \boldsymbol{\theta}) + \sum_{i=r+s+1}^n\log p (x_i, z_i | \boldsymbol{\theta})\right)  \right\}}
\end{align}
\noindent Keep in mind that we assumed that the variables were discrete in the begin, and as such if the variables were continuous then the summations would be replaced by integrals as the following. The extension to multiple variables is also relatively straightforward \citep{allison1999missing}. In order to implement a maximum likelihood method on data that contains missing values its important to have a model for the joint distribution for all variables in the data set, and accordingly have a numerical method for maximizing the likelihood of this distribution. Determining this model can vary with the type of data that one is dealing with.\\
\indent In our data set, we have both continuous and indicator variables. When the data is continuous its common to assume a multivariate-normal model, i.e. that all the variables are independently identically normally distributed (iid) and can be expressed as a linear function of all other variables (or subsets). There is also an assumption that the errors are homoscedastic, i.e. constant and have a mean of 0. In the case of the indicator variables, its difficult to assume that these variables are normally distributed. However, according to \cite{schafer1997analysis}, \cite{schafer1998multiple} and \cite{allison1999missing} simulation evidence and practical experience have shown that maximum likelihood methods can do a god job in imputing missing values even if the variables in question are indicator variables. Still, we have adopted to use a different imputation methods for each of the types of data, i.e. we use a bootstrapped expectation-maximization (EM) imputation method for the variables that are continuous and a classification- and regression tree (CART) based imputation method for the indicator variables.\\
\indent As we mentioned one needs to have a numerical method for maximizing the likelihood of the joint probability distribution. One of the most common numerical methods for doing just this is the expectation-maximization (EM) algorithm \citep{dempster1977maximum}. We mentioned it slightly in subsection (\ref{subsec:little}), but it is an iterative algorithm that is used to maximize the likelihood function (eq. \ref{eq:likelihood}) of a number of missing data models. It is comprised of two steps, the expectation step (often called the $E$ step) and the maximization step (called the $M$ step). In the expectation step the expected values of the log-likelihood is taken over the variables with missing values using the current estimated parameters \citep{allison1999missing}. After this is done the maximization step involves maximizing the expected log-likelihood in order to get new estimates of the parameters. These two step are continued until convergence is achieved, i.e. until the estimated parameters of the joint probability distribution do not change from one iteration until the next. Most standard software packages using an EM implementation have as a principal output a set of maximum likelihood parameters related to the joint probability distribution. The imputed values are often included in addition, but are not recommended for further analysis. The reason for this is that these imputed values are not designed for that purpose and as such will produce biased estimates of many parameters if used in further analysis \citep{allison1999missing}.\\
\indent A way to get around this problem is by using multiple-imputation. \cite{honaker2011amelia}  introduced a bootstrapped EM algorithm that combines the nice properties of the EM algorithm, i.e. consistency, asymptotic efficiency etc. with the accuracy property of the bootstrap re-sampling method, see \cite{efron1992bootstrap} and \cite{james2013introduction}. \cite{honaker2011amelia} also argue that the EMB algorithm they developed is much faster and more reliable than alternative algorithms. In addition to making valid and much more accurate imputations for cross-sectional data. The algorithm is implemented in the \texttt{Amalie II} package in \texttt{r}. The assumptions of the algorithm are: If we assume that the data set can be expressed as a matrix $\boldsymbol{D}$ consisting of dimensions $(n\times k)$. Let the matrix $\boldsymbol{D}$ be comprised of two parts, i.e. $\boldsymbol{D^{mis}}$ the missing part and $\boldsymbol{D^{obs}}$ the observer part. The matrix $\boldsymbol{D}$ is assumed to follow a multivariate distribution with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$. This assumption can be stated as $\boldsymbol{D} \sim N\left(\boldsymbol{\mu},  \boldsymbol{\Sigma}\right)$. In addition to the multivariate normality assumption, the algorithm assumes that the data is MAR. The latter have we already shown to be intact, but the first assumption is somewhat difficult. As the data is by definition incomplete due to the missing data, we assume that this assumption intact. Typically one would test if this assumption is intact using a multivariate normality test similar to the ones mentioned by \cite{mardia1970measures}, \cite{henze1990class} or \cite{royston1982extension}. Most of these tests assume that the data is complete, and should the data be incomplete then its common to remove the missing observations and conduct the tests on the remaining data. The challenge for our part is that approximately 15\% of our data set is missing which may cast doubt on the loss of statistical power that these tests may have. As a result of this we have chosen to assume that the normality assumption is intact. The schematic approach of this algorithm and the way it used in this thesis is described in Figure (\ref{fig:BEM_algo}). The procedure starts by producing $n$ bootstrapped data sets for which the EM algorithm is run on each bootstrapped data sets. In the full data sample (\texttt{HFfulldata set.Rdat}) we let the algorithm produce $n = 20$ bootstrapped data sets, whilst for the other data sets, i.e. \texttt{HFpEFdata set.Rdat} and \texttt{HFmrEFdata set.Rdat} we use $n = 100$. After the imputed data sets are produced they are collapsed by averaging all the imputed values produced by the EM algorithm. All the data from the incomplete data set that the procedure started with should be the same with the exception of the missing values, i.e. these have be replaced by the average of the imputed values.

\input{doc/thesis/images/BEM.tex}

\indent For the indicator variables, the imputation technique is defined by a classification- and regression tree (CART) algorithm. This algorithm is implemented in the \texttt{mice} package in \texttt{r} \citep{buuren2010mice}. The implementation proceeds as follows: for each variable $\boldsymbol{k}$ in the matrix $\boldsymbol{D}$, the algorithm fits a classification or regression three by recursive partitioning. Then for each missing value in $\boldsymbol{k}$ the algorithm finds the terminal nodes, i.e. the nodes the missing value can end up in according to the fitted tree. Lastly, the algorithm makes a random draw among the members in the nodes, and takes the observed value from that draw as the imputation. Rather than collapsing the multiple imputed data sets as with the BEM algorithm, we simply use the first imputed data sets for further analysis. For further information about the procedure of this algorithm please see. \cite{burgette2010multiple}.\\
\indent 



\newpage


\subsection{Dimensional Reduction}
\label{subsec:dim_red}

\section{Clustering Patient Groups}
\label{sec:cluster_pat_gro}

\subsection{Hierarchical}
\label{subsec:hierarchical}

\subsection{k-Means}
\label{subsec:k-means}

\subsection{Expectation-Maximization}
\label{subsec:em}

\section{Classifying Clinical Outcomes}
\label{sec:classify_clin_out}

\subsection{k-Nearest Neighbours (k-NN)}
\label{subsec:knn}

\subsection{Support Vector Machines (SVM)}
\label{subsec:svm}

\subsection{Random Forrest}
\label{subsec:random_forr}

\subsection{LASSO}
\label{subsec:lasso}

\section{Validation}
\label{sec:validation}

\subsection{k-Fold}
\label{subsec:k_fold}

\subsection{Leave One Out}
\label{subsec:loocv}

\end{document}