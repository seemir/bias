\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Methodology}
\label{chap:method}

\noindent In this chapter we present the methodology and research structure used in this thesis. Some pre-processing of data including imputation and dimensional reduction will also be explained and presented. The implementation of the ML algorithms that produce the results presented in chapter (\ref{chap:exp}).

\section{Overview}

\noindent As stated in the chapter (\ref{chap:intro}), the aim of the thesis is split into two parts. The first part being that of seeing how well various clustering methods perform in producing phenotypically distinct clinical patient groups with HFpEF and HFmrEF? We frame the SL problem in the setting of unsupervised learning and accordingly use the following clustering methods: hierarchical clustering, expectation-maximization and latent class analysis to evaluate which produce the most clinically useful patient groups. The use of these clustering methods are common in the literature (see section \ref{subsec:unsupervisedlearn}) and serves as the main motivation for including them in our analysis. The second part of the problem statement looks at evaluating the accuracy of various classification algorithms in predicting the mortality and re-admission of patients with HFpEF and HFmrEF? In accordance with the literature as presented in section (\ref{sec:predclincout}), we reduce the SL problem of predicting the mortality and re-admission into a two class classification problem where both classes of outcomes are whether or not mortality/re-admission  \input{doc/thesis/images/Research_structure.tex}\noindent occurred. The classification methods that we will be evaluating are: k-nearest neighbours (k-NN), support vector machines (SVM), random forest and least absolute shrinkage and selection operator (LASSO) algorithms. All the algorithms are much used in the literature. The motivation behind the use of the chosen algorithms has always been to confirm with the practices done in the literature. We do however need to emphasize that many algorithms exists that can be used to further broaden the analysis done in this thesis. This is something we have not done due to limitations.\\
\indent The machine learning procedure adopted in this thesis is illustrated in figure (\ref{fig:ML_proc_thesis}). The structure starts by pre-processing the data. This includes imputation of missing data to ensure that the data is balanced and dimensional reduction to address eventual problems with higher dimensional multi-correlated variables. Both the pre-processing steps are explained in further detail later in this chapter (see section \ref{sec:data}). After the pre-processing is done the structure continues by first addressing the cluster analysis. Being that the dimension reduction is relevant for both the cluster analysis and classification. We use the components derived from the dimension reduction process as input into both the clustering and classification algorithms evaluated. The cluster analysis runs the produced components through the three cluster algorithms (hierarchical clustering, expectation maximization and latent component analysis). After the procedure is done, three sets of clusters are produced and the next step is to evaluate the clusters to assess their medical usefulness. The supervised classification track on the other hand is structured in a somewhat different way. First the procedure starts by removing the labels of the various clinical outcomes associated with the given patients. After this is done the chosen components from the dimension reduction is run through the four classification algorithms (k-NN, SVM, RF and LASSO) and the data is trained and validated to produce approximately unbiased estimates of the test errors/accuracy. After the data is run thought the classification process and the accuracy are produced, the algorithms are ranked and evaluated accordingly. The outputs of the whole ML procedure are i) clinical clusters with (maybe) distinct phenotypical properties and ii) the accuracy of the various classification algorithms in predicting re-admission and mortality in the data sets.\\
\indent All the processes mentioned in the ML procedure in figure (\ref{fig:ML_proc_thesis}) are developed using the \texttt{R} statistical programming language (version 3.4.4 - \textit{Someone to Lean On}) \citep{Rsoftware2018} with RStudio as the integrated development environment (IDE), version 1.1.423 \citep{RStudio2018}. We use a number of external libraries and self-made algorithms in order to make the process more efficient. Data description with variable explanations, descriptive statistics and some relevant plot can be fount in appendix (\ref{chap:data_desc}). The source code used to produce all the results in this thesis can be found in appendix (\ref{chap:souce_code}). As we now have given an overview of the ML procedure used in this thesis we move on to presenting the data.

\section{Data}
\label{sec:data}

\noindent The data used is comprised of two data sets (\texttt{data\_use\_HFpEF.mat}, $N = 182$ and \texttt{data\_use\_HFmrEF.mat}, $N = 193$). The data was collected by the medical staff at a tertiary hospital in the United Kingdom. At this particular hospital NT-proBNP led heart failure service were run on all patients with suspected heart failure. All patients with suspected HF based on an assessment of the HF probability and raised NT-proBNP/BNP levels (see figure \ref{fig:esc_algo_hf}) were included and forwarded for an echocardiography. An expert HF physician reviewed all the patients after the echocardiography was performed. The patients were diagnosed with HF according to the 2016 ESC guidelines \citep{ponikowski2016}. Accordingly, signs and symptoms of HF, raised NP values, echocardiographic results including left ventricular ejection fraction (LVEF) and evidence of structural or functional heart abnormalities were the primary basis for the assessment done by the hospitals cardiac physicians. After the diagnosis patients were categorized based on LVEF following the ESC guidelines, i.e. patients with LVEF $>$ 50\% were classified as HFpEF and those with $40 \leq$ LVEF $< 50$ as HFmrEF. The patients with LVEF $<$ 40\%, greater than moderate valvular heart disease and prior cardiac transplantation were excluded. The data was collected over a one year period from October 10th, 2014 to October 9th, 2015. In total 375 patients were analyzed over this one year period with data from 126 clinical features being recorded. The outcomes were evaluated through the hospitals databases and mortality was confirmed with the Office for National Statistics. All the data was collected as part of the hospitals approved Clinical Audit.\\
\indent As mentioned in the previous section, we reduced the SL problem in the supervised learning part of the ML procedure to that of a two class classification problem. The way in which this was done was with respect to the various \texttt{patient\_groups} in the data. The patients were grouped based on various outcomes. In total six outcome categories were defined in the data sets. The outcome categories are as follows: \texttt{IN} - inhospital death, \texttt{Z} -

\input{doc/thesis/tables/Outcome_classes.tex}

\noindent dead within 30 days, \texttt{Y} - dead within 1 year, \texttt{X} - dead by Fluorouracil (medication), \texttt{V} - cardiac readmission within 30 days, \texttt{U} - readmission and \texttt{R} - the rest. The various combinations of the outcome classes found in the data sets and the way in which they were classified are listed in table (\ref{tab:outcomes_class}). From this table we can see that approximately 36\% of all the patients in the HFpEF data set were readmitted in some form, i.e either within 30 days or more. In the HFmrEF data set this number was somewhat smaller being approximately 23\%. The number also differed with respect to the whether the patients were confirmed deceased or not. In the HFpEF data set approximately 30\% of the patients were confirmed deceased and in the HFmrEF data set this number was 31\%. Further descriptive statistics on the data can be found in appendix (\ref{sec:desc_stat}). The source code for the two-class outcome classification shown in table (\ref{tab:outcomes_class}) can be found in appendix (\ref{sec:app_desc_stat}).\\
\indent As the data used in this thesis is cross-sectional, we need to emphasize that it is not perfect. Limitations to the data sets are many and one of the most primary of them is in regard to missing data.


\subsection{Missing Data}
\label{subsec:miss_data}

\noindent Missing values in data is a very important concept in data management and a highly prevalent problem in any data analysis. If one does not handle missing values properly this may lead to inaccurate or invalid inference being drawn from the data. Results where improper treatment of missing data is present may differ significantly from those where missing data is not present. In medical research it is not uncommon for data about a given patient to be missing. One typically considered missing data from patients clinical variables as being the values that are not directly observed \citep{ibrahim2012missing}. Data can be missing due to a number of reasons. In clinical research some reasons may include: poor communication with study subject, difficulties assessing the clinical outcomes, 
lack of consolidation from test, duration of trial etc. The latter is often a reason for missing data as longer trials tend to produce more risk of missing data. Especially considering that patients often run the risk of being dropped from the study before completion \citep{myers2000handling}.\\
\indent In our data sets the problem with missing values is very present. In the HFpEF data set a total of 2383 observations, i.e. approximately 13.3\% of the data set is missing. In the non-indicator variables, the largest contributors to this can be attributed to the lack of glucose measured (\texttt{glucose}, 5.7\%), failure to register time to HF admission (\texttt{timetohfadm}, 5.2\%), ferritin levels (\texttt{ferritin}, 5.1\%), lateral peak velocity of systolic pulmonary vein flow (\texttt{laterals}, 5.0\%), glycohemoglobin (\texttt{hba1c}, 4.7\%) and transferrin saturation (\texttt{tsat}, 4.2\%). These variables contribute to approximately 29.9\% of the total 

\input{doc/thesis/tables/Missing_values.tex}
\newpage
\noindent missing values. In the HFmrEF data set, the picure is very much different. In general we can say that this data set has a much larger presence of missing values. In total 3586 observations, i.e. approximately 22.4\% of the data is missing. The largest non-indicator contributors are: inability to record the body mass index (BMI) at admission (\texttt{bmiadmission}, 5\%), the weight after patients are discharged (\texttt{dischargeweight}, 4.9\%), measures of troponin (\texttt{troponin}, 3.9\%), the time to first cardiac hospitalization (\texttt{timetofirstcardiachospitalisation}, 3.9\%) and the patients weight at admission (\texttt{admissionwgt}, 3.7\%).


\subsection{Imputation}
\label{subsec:impu}

\subsection{Dimensional Reduction}
\label{subsec:dim_red}

\section{Clustering Patient Groups}
\label{sec:cluster_pat_gro}

\subsection{Hierarchical}
\label{subsec:hierarchical}

\subsection{Expectation-Maximization}
\label{subsec:em}

\subsection{Latent Class Analysis}
\label{subsec:lca}

\section{Classifying Clinical Outcomes}
\label{sec:classify_clin_out}

\subsection{k-Nearest Neighbours (k-NN)}
\label{subsec:knn}

\subsection{Support Vector Machines (SVM)}
\label{subsec:svm}

\subsection{Random Forrest}
\label{subsec:random_forr}

\subsection{LASSO}
\label{subsec:lasso}

\section{Validation}
\label{sec:validation}

\subsection{k-Fold}
\label{subsec:k_fold}

\subsection{Leave One Out}
\label{subsec:loocv}

\end{document}