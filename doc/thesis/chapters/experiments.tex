\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Experiments}
\label{chap:exp}

\noindent In this chapter, we present the results from the experiments that were done in this thesis. The results are split into two sections, i.e. the results from the cluster analysis and the classification sub procedure mentioned in figure (\ref{fig:ML_proc_thesis}). For each of the sections we present an overview of the statistical problems that the algorithms are to solve. In this, we also present the assumptions and the evaluation criteria that are used to rank the algorithms given the statistical learning problem that they need to solve. 

\section{Cluster Analysis}

\noindent In the cluster analysis, we try to see how well the various clustering algorithms perform in producing phenotypically distinct clinical patient groups with HFpEF and HFmrEF. We will organize this section in the following way: we start out by looking at the full sample data set, i.e. HFfullDataSet.Rdat. After the pre-processing, we will run the principal components thought the clustering algorithms. The idea is to see how well the clustering algorithms perform in producing patient groups that are more unique compared to the physicians evaluation. Our measure of success is the number of unique baseline characteristics that are statistically significant using the Person c2 test for categorical variables, ANOVA for normally distributed variables and Kruskalâ€“Wallis test for non-normally distributed variables \citep{kruskal1952use}. Significance
used is that of the conventional 5\% level. The implementation is done using the \texttt{multigrps}-function from the \texttt{CBCgrps}-package in \texttt{r} \citep{CBCgrps}. The algorithms are first going to be performed on the BI clustering HF problem, i.e. see how unique the patient groups produced are given that the only HF subtypes in the dataset is HFmrEF and HFpEF. After this is done, we will see how well the algorithms will perform in producing new clusters within the already defined patient groups from the first round. We will do the same analysis on both the groups that have been defined by the physicians and the first round clustering results.

\subsection{The BI Clustering HF Problem}
\label{subsec:bi_clust}

In the current clustering problem we assume that the dataset is comprised of two clusters HFmrEF and HFpEF. Accordingly, we allow the algorithms to determine the patients that best correspond to each group. We have plotted the results of the BI clustering problem in Figure (??). This plot can in many ways seem very misguiding as it only displays the results along the two first principal components. Still, as we can see from the table below, even if you only cluster based on the first four principal components (27.32\% of variance explained), one can produce more unique phenotypically distinct patient groups than the physicians. As we can see from table (\ref{tab:baseline_char_actual_full}), the 

\input{doc/thesis/tables/baseline_char_actual_full.tex}

\input{doc/thesis/tables/baseline_char_HCKM_full.tex}

\noindent Hierarchical and K-Means clustering algorithms both give the highest number of significant
baseline characteristics (7 for only the cont. variables in the table and 62 in total) compare with the actual clustering done by the physicians (4 for the cont. variables and 59 in total). The EM algorithm produces overall the lowest number of significant baseline characteristics (5 for cont. variables and 54 in total). Both the Hierarchical and K-Means algorithm produce the same clustering configurations. The baseline characteristics in the clustering of the patient using the Hierarchical and K-Means clustering show that for the first clustering (HFpEF) the \texttt{LVEF} are on average 57.5\% and
for the second clustering (HFmrEF) the \texttt{LVEF} are on average 45\%. These are very similar values as that which was done by the physicians. We can also see for other baseline characteristics such as \texttt{ntprobnp} the average is at 2327 ng/L for the HFpEF group which is significantly different than that of the HFmrEF group 3723.5 ng/L, also this is very similar to what the physicians concluded with. For characteristics that are significantly different in the clustering with Hierarchical and K-Means clustering, but not done so for the physicians one can include the following continuous variables: hemoglobin (\texttt{hb}), packed cell volume (\texttt{pcv}) and the ewave (\texttt{ewave}). This may suggest that both the clustering algorithms can be used as appropriate tools for physicians. The results from the EM algorithm 

\input{doc/thesis/tables/baseline_char_EM_full.tex}

\noindent (Table \ref{tab:baseline_char_actual_em}) show that a lot of the similar baseline characteristics are not statistically significant. The LVEF (\texttt{lvef}) and NTproBNP (\texttt{ntprobnp}) is very similar to both the Hierarchical and K-means clustering, but other characteristics such as hemoglobin (\texttt{hb}) and the packed cell volume (\texttt{pcv}) are not. Throughout the analysis we have found that the EM algorithm does not a good job of clustering patient groups compared to the Hierarchical and K-Means clustering algorithms. This could be because of the assumption of multivariate normal distribution does not hold for this data set.

\subsection{Clustering with and without Post-Diagnosis}

\noindent In this section we will investigate the clustering results discussed previously. We will place an assumption of weather the physicians diagnosis is representative given an objective of producing the most unique patient groups. The clustering problem in this section assumes that the diagnosis done by the physicians is sufficient in regards to this objective, i.e. the clustering based on the \textit{post-diagnosis} done by the physicians produces the most unique patient groups. We will compare these results to a clustering without an assumption of post-diagnosis done by the physicians and see if 

\input{doc/thesis/tables/results_clustering_subtypes.tex}

\noindent there are any substantial differences in results. We will only use the first two principal components (14.64\% of variance explained) to cluster the patients. The evaluation criteria is the same as in the previous section. We can see from table (\ref{tab:n_baseline}) that the Hierarchical and K-Means clustering algorithms almost always produce the same number of significant baseline characteristics. The only exception seems to be were one does not assume a post-diagnosis of the HFmrEF subgroup. The overall algorithm that produces the lowest number of significant baseline characteristics is the Expectation maximization algorithm. Starting with the algorithms performance given the assumption of post-diagnosis. We can see from table (A3) and (A4) that for the case with HFpEF, cluster 2 seems to contain patients that have a higher average \texttt{age} (85.45) with a packed cell volume (\texttt{pcv}) that is on average 0.33 $\pm$ 0.05. Given this information this puts cluster 2 right in the middle of clusters 1 and 3. The ntprobnp (\texttt{ntprobnp}) of cluster 3 is the lowest at 1417 ng/L which is also statistically significant. The average number of red blood cells, i.e. the mean corpusular volume (\texttt{mcv}) is on average 87 femtolitres which is less than clusters 2 and 3. Overall, we have 8 continuous significantly different baseline characteristics with 47 being categorical (each significant category is counted independently). The EM algorithm produced almost similar results for the subgroup HFpEF (table A.5). The second cluster produced by the EM algorithm is very similar to the first cluster produced by the Hierarchical and K-Means algorithm. The ntprobnp (\texttt{ntprobnp}) for cluster one and two produced by the EM are very similar. Both are approximately 2750 ng/L. The third cluster has the lowest values for the ntprobnp (1525 nl/L). Accordingly, we can see that cluster 3 produced by the EM algorithm is very similar to the third cluster produce by the Hierarchical and K-Means algorithms. The total number of significant baseline characteristics for the EM algorithm is 53 (8 cont. and 45 categorical). When looking at the HFmrEF clustering based on post-diagnosis, we can see that a somewhat different results shows up, i.e. there are less significantly different baseline characteristics. For cluster 3, we see that the lowest ntprobnp (\texttt{ntprobnp}) at 2898.5 ng/L with a packed cell volume of 0.38 $\pm$ 0.04. This cluster also contains the patients with the lowest length of stay (7 days). The length of stay (\texttt{LOS}) is also a uniquely statistical significant baseline characteristic that is only significant in the HFmrEF subgroup of patients. This cluster also has the highest hemoglobin (\texttt{hb}) at 23.79 $\pm$ 12.89 g/100mL. Comparing the number of significant baseline characteristics between the HFmrEF groups both with and without the post-diagnosis assumption one can see that the latter has fewer in the case with the assumption, see table (\ref{tab:n_baseline}). The same goes for the HFpEF group, i.e. we have reasons to believe that assuming the physicians diagnosis is representative one can get additional clustered patient groups with higher degree of homogeneity compared to when this assumption is not intact. We have also demonstrated that the ML algorithms can be very useful in producing patient groups that are more phenotypically unique given that the objective is to challenge the diagnosis of the physicians, see section (\ref{subsec:bi_clust}). Now that we have presented the results of the clustering analysis, we move on to the results of the classification of the clinical outcomes. The source code, relevant plots and tables can be found in the appendix (\ref{chap:data_desc}).

\section{Classification}

\noindent In this section we will present the results of the classification analysis. As mentioned in ML the procedure (figure \ref{fig:ML_proc_thesis}), we run the imputed data set through the various classification algorithms and accordingly run a 10-fold cross validation in order to estimate the accuracy of the various algorithms. The accuracy along with Cohenâ€™s kappa are the two evaluation criteria we use to rank the algorithms in this section.

\subsection{Mortality Classifier}

\noindent The statistical learning problem in this section is given by a two-class
classification problem where mortality is the clinical outcome in question.

\subsection{Re-admission Classifier}

\section{Discussion}

\end{document}